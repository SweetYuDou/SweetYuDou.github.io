<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>读RE-Net后有但不多的一些理解</title>
    <url>/2024/11/01/RE-Net/</url>
    <content><![CDATA[<h3 id="《Recurrent-Event-Network-Autoregressive-Structure-Inference-over-Temporal-Knowledge-Graphs》论文解读"><a href="#《Recurrent-Event-Network-Autoregressive-Structure-Inference-over-Temporal-Knowledge-Graphs》论文解读" class="headerlink" title="《Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs》论文解读"></a>《Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs》论文解读</h3><h4 id="RGCN"><a href="#RGCN" class="headerlink" title="RGCN"></a>RGCN</h4><p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238bf47b.png" alt="RGCN_formula"></p>
<p>W<sub>r</sub><sup>(l)</sup>是某一个特定的关系r（如弹劾，主动建交等）在第l层GCN中的权重矩阵，每一个不同的关系r对应其对应的权重矩阵，比如r<sub>1</sub>对应W<sub>1</sub>这样</p>
<p>h<sub>o</sub><sup>(l)</sup>是某一个特定的客体（object）在第l层GCN中的隐藏表示&#x2F;特征向量（hidden state&#x2F;embedding），也就是当前s实体所指向的那个o客体，<em><strong>h<sub>s</sub><sup>(l)</sup>同理</strong></em></p>
<p>W<sub>o</sub><sup>(l)</sup>是自环权重矩阵，意思就是它会与节点s自身的特征一直相乘然后更新到下一层再继续相乘</p>
<p>N<sub>t</sub><sup>(s,r)</sup>表示在时间戳t（timestamp）这个时间下的节点s通过关系r的邻居节点集合（Neighborhood），<em><strong>N<sub>t</sub><sup>(s)</sup>同理</strong></em></p>
<p>c<sub>s</sub>是归一化常数，体现在公式里就是：num(o) * num(r)，就是当前t的实体s对应的客体o的个数乘上当前t这个s对应的关系r的个数，包含了多少个邻居的信息就平均掉。<strong>简单来说，就是当前t的实体s的所有邻居个数</strong></p>
<p>RGCN聚合器的作用在于：聚合节点s的邻居节点的信息。结合公式解读一下就是：对于节点s，RGCN在更新节点特征同时考虑<strong>节点自身的特征</strong><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238ab7d7.png" alt="self_loop">和<strong>其邻居节点的特征</strong><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238b540f.png" alt="self_rel">，以及它们之间的关系类型（指不同关系对应不同权重矩阵)。</p>
<h4 id="RGCN模块代码如下："><a href="#RGCN模块代码如下：" class="headerlink" title="RGCN模块代码如下："></a>RGCN模块代码如下：</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RGCNLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_feat, out_feat, bias=<span class="literal">None</span>, activation=<span class="literal">None</span>, self_loop=<span class="literal">False</span>, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RGCNLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.bias = bias</span><br><span class="line">        <span class="variable language_">self</span>.activation = activation</span><br><span class="line">        <span class="variable language_">self</span>.self_loop = self_loop</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            <span class="variable language_">self</span>.bias = nn.Parameter(torch.Tensor(out_feat))</span><br><span class="line">            nn.init.xavier_uniform_(<span class="variable language_">self</span>.bias,  gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">            <span class="comment"># xavier_uniform_：初始化，用于保证输入输出的方差相同。可以避免随着层数的传递，输入过大而梯度消失或输入过小而失去非线性</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 权重矩阵</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">            <span class="comment"># 回环权重矩阵</span></span><br><span class="line">            <span class="comment"># input_feature,output_feature</span></span><br><span class="line">            <span class="variable language_">self</span>.loop_weight = nn.Parameter(torch.Tensor(in_feat, out_feat))</span><br><span class="line">            nn.init.xavier_uniform_(<span class="variable language_">self</span>.loop_weight, gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dropout:</span><br><span class="line">            <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dropout = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            g是graph，.ndata[&#x27;h&#x27;]是PyTorch DGL图（Graph）对象的一个节点数据属性，它表示图中的节点特征向量。</span></span><br><span class="line"><span class="string">            在 GNNs 中，节点特征向量通常由两部分组成：一部分是静态的特征，如节点类别、属性等；</span></span><br><span class="line"><span class="string">            另一部分是动态的特征，如与节点相邻的节点数量、节点之间的边权重等。</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            loop_message = torch.mm(g.ndata[<span class="string">&#x27;h&#x27;</span>], <span class="variable language_">self</span>.loop_weight)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                loop_message = <span class="variable language_">self</span>.dropout(loop_message)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.propagate(g, reverse)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用bias偏移和激活函数</span></span><br><span class="line">        node_repr = g.ndata[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            node_repr = node_repr + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">            node_repr = node_repr + loop_message</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.activation:</span><br><span class="line">            node_repr = <span class="variable language_">self</span>.activation(node_repr)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新节点特征向量</span></span><br><span class="line">        g.ndata[<span class="string">&#x27;h&#x27;</span>] = node_repr</span><br><span class="line">        <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RGCNBlockLayer</span>(<span class="title class_ inherited__">RGCNLayer</span>):  <span class="comment"># RGCN块层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_feat, out_feat, num_rels, num_bases, bias=<span class="literal">None</span>, activation=<span class="literal">None</span>, self_loop=<span class="literal">False</span>, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RGCNBlockLayer, <span class="variable language_">self</span>).__init__(in_feat, </span><br><span class="line">                                             out_feat, </span><br><span class="line">                                             bias,</span><br><span class="line">                                             activation, </span><br><span class="line">                                             self_loop=self_loop,</span><br><span class="line">                                             dropout=dropout)</span><br><span class="line">        <span class="variable language_">self</span>.num_rels = num_rels</span><br><span class="line">        <span class="variable language_">self</span>.num_bases = num_bases</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.num_bases &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.out_feat = out_feat</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.submat_in = in_feat // <span class="variable language_">self</span>.num_bases</span><br><span class="line">        <span class="variable language_">self</span>.submat_out = out_feat // <span class="variable language_">self</span>.num_bases</span><br><span class="line"></span><br><span class="line">        <span class="comment"># assuming in_feat and out_feat are both divisible by num_bases</span></span><br><span class="line">        <span class="comment"># if self.num_rels == 2:</span></span><br><span class="line">        <span class="comment">#     self.in_feat = in_feat</span></span><br><span class="line">        <span class="comment">#     self.weight = nn.Parameter(torch.Tensor(</span></span><br><span class="line">        <span class="comment">#         self.num_rels, in_feat, out_feat))</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.Tensor(<span class="variable language_">self</span>.num_rels, <span class="variable language_">self</span>.num_bases * <span class="variable language_">self</span>.submat_in * <span class="variable language_">self</span>.submat_out))</span><br><span class="line">        nn.init.xavier_uniform_(<span class="variable language_">self</span>.weight, gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">msg_func</span>(<span class="params">self, edges, reverse</span>):  <span class="comment"># 节点和权重相乘，再转化为输出格式</span></span><br><span class="line">        <span class="keyword">if</span> reverse:</span><br><span class="line">            <span class="comment"># edges.data[&#x27;type_o&#x27;]：表示目标节点所属的关系的类型。具体哪个关系就对应它的权重矩阵</span></span><br><span class="line">            weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_o&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_s&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        edges.src[&#x27;h&#x27;]：表示源节点的特征向量。</span></span><br><span class="line"><span class="string">        它是一个大小为 (num_nodes, submat_in) 的tensor，其中 num_nodes 表示图中的节点数。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        node = edges.src[<span class="string">&#x27;h&#x27;</span>].view(-<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.submat_in)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        msg：表示消息传递的结果。</span></span><br><span class="line"><span class="string">        它是一个大小为 (num_edges, out_feat) 的tensor，其中out_feat 表示输出特征向量的维度。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        msg = torch.bmm(node, weight).view(-<span class="number">1</span>, <span class="variable language_">self</span>.out_feat)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;msg&#x27;</span>: msg&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">        g.update_all(<span class="keyword">lambda</span> x: <span class="variable language_">self</span>.msg_func(x, reverse), fn.<span class="built_in">sum</span>(msg=<span class="string">&#x27;msg&#x27;</span>, out=<span class="string">&#x27;h&#x27;</span>), <span class="variable language_">self</span>.apply_func)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply_func</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;h&#x27;</span>: nodes.data[<span class="string">&#x27;h&#x27;</span>] * nodes.data[<span class="string">&#x27;norm&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure></div>

<p>结合代码来看公式，是<img lazyload src="/images/loading.svg" data-src="/RE-Net/67376238ab7d7.png" alt="self_loop"></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">   		<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         先让节点特征向量与自环权重矩阵相乘+dropout</span></span><br><span class="line"><span class="string">         &#x27;&#x27;&#x27;</span></span><br><span class="line">loop_message = torch.mm(g.ndata[<span class="string">&#x27;h&#x27;</span>], <span class="variable language_">self</span>.loop_weight)</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">             loop_message = <span class="variable language_">self</span>.dropout(loop_message)</span><br><span class="line">         <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         再把偏移、乘积、激活函数加在原有的节点特征向量上</span></span><br><span class="line"><span class="string">         允许每一层都保留前一层的信息，有点像ResNet的思想</span></span><br><span class="line"><span class="string">         &#x27;&#x27;&#x27;</span></span><br><span class="line">	node_repr = g.ndata[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">             node_repr = node_repr + <span class="variable language_">self</span>.bias</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">             node_repr = node_repr + loop_message</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.activation:</span><br><span class="line">             node_repr = <span class="variable language_">self</span>.activation(node_repr)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把最后的节点表示（node_repr）作为该层节点特征向量的输出</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">         g.ndata[<span class="string">&#x27;h&#x27;</span>] = node_repr</span><br><span class="line">         <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure></div>

<p><img lazyload src="/images/loading.svg" data-src="/RE-Net/67376238b540f.png" alt="self_rel">是</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">def</span> <span class="title function_">msg_func</span>(<span class="params">self, edges, reverse</span>): </span><br><span class="line">      <span class="keyword">if</span> reverse:</span><br><span class="line">          weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_o&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_s&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">先找到目标节点所属的关系的类型（比如：弹劾、被弹劾；选举、被选举）</span></span><br><span class="line"><span class="string">拿到该关系所对应的权重矩阵</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">      node = edges.src[<span class="string">&#x27;h&#x27;</span>].view(-<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.submat_in)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将源节点的特征向量与权重相乘</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">      msg = torch.bmm(node, weight).view(-<span class="number">1</span>, <span class="variable language_">self</span>.out_feat)</span><br><span class="line">      <span class="keyword">return</span> &#123;<span class="string">&#x27;msg&#x27;</span>: msg&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">      g.update_all(<span class="keyword">lambda</span> x: <span class="variable language_">self</span>.msg_func(x, reverse), fn.<span class="built_in">sum</span>(msg=<span class="string">&#x27;msg&#x27;</span>, out=<span class="string">&#x27;h&#x27;</span>), <span class="variable language_">self</span>.apply_func)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把该源节点的所有邻居节点的msg结果相加聚合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">apply_func</span>(<span class="params">self, nodes</span>):</span><br><span class="line">      <span class="keyword">return</span> &#123;<span class="string">&#x27;h&#x27;</span>: nodes.data[<span class="string">&#x27;h&#x27;</span>] * nodes.data[<span class="string">&#x27;norm&#x27;</span>]&#125;</span><br><span class="line">  	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  	最后乘上归一化常数nodes.data[&#x27;norm&#x27;]</span></span><br><span class="line"><span class="string">  	&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div>

<p>RGCN捕捉多跳邻居的信息：通过多次应用图卷积操作实现</p>
<ul>
<li>在第一次迭代中，每个节点聚合来自其直接邻居的信息</li>
<li>在第二次迭代中，每个节点合来自其邻居的邻居（即第二跳邻居）的信息</li>
</ul>
<h4 id="RE-Net架构图解读"><a href="#RE-Net架构图解读" class="headerlink" title="RE-Net架构图解读"></a>RE-Net架构图解读</h4><p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238ebfd3.png" alt="RE-Net_architecture"></p>
<p>任务：预测时间戳t中的三元组</p>
<p>已知：过去几个时间步的图（架构图举例为3个时间步）</p>
<p>RE-Net_global对整个知识图谱（能观察到的图，架构图中整个图谱为3个时间步的图）进行编码，给出全局嵌入表示（global_emb）。在预测时给出预测时间戳t的主体、客体分布以及更新后的全局嵌入表示</p>
<p>RE-Net(train)：将可观察到的局部图中的s,r和global_emb等放入Aggregator聚合器（RGCN）当中，聚合器的输出结果为两个序列s_packed_input,s_packed_input_r分别放入encoder,encoder_r中，编码实体表示和关系表示，RE-Net类的self.encoder被定义为nn.GRU，最后分别送入线性层self.linear,self.linear_r(nn.linear)和dropout层预测客体和关系并分别计算损失</p>
<p>RE-Net(valid,test)：RE-Net_global预测时间戳t的主体，客体分布后采样前self.num_k个，更新（主体和客体的列表、索引、历史交互缓存），构建新的时间点的图，更新图字典（graph_dict）和全局嵌入表示（global_emb）。再重复以上操作↑</p>
]]></content>
      <categories>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>玩Docker后有但不多的一些心得</title>
    <url>/2024/11/14/Docker/</url>
    <content><![CDATA[<ul>
<li>docker镜像源（加速地址）是国内的源来拉镜像，可能有一些镜像没有，代理就是从Docker hub拉。<strong>你拉的镜像如果是<em>Dockerhub</em>的，并且国内镜像源没有，那镜像源也不好使，只能老老实实配合代理</strong>。阿里云ECS可以使用阿里云镜像加速器，<code>/etc/docker/daemon.json</code>如下配置即可，不要加其他源：</li>
</ul>
<div class="highlight-container" data-rel="Json"><figure class="iseeu highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;registry-mirrors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;https://***.mirror.aliyuncs.com&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>本地访问阿里云ECS的网页时，若出现无法访问的问题，可以检查该ECS配置的安全组是否开放HTTP（80）、HTTPS（443）端口。</li>
<li>用clash来给服务器<strong>配置代理</strong>，可以参考<a class="link" href="https://github.com/nelvko/clash-for-linux-install?tab=readme-ov-file">nelvko&#x2F;clash-for-linux-install: 优雅地部署基于 Clash 的代理环境。 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>，其中订阅就是一个链接，可以从机场网站上找，类似这样的：</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/673762390d8ea.png" alt="subscription"></p>
<p>我这里用的是clash，就复制clash订阅链接就好了。部署完clash后，服务器开放端口7890作为代理接口，来访问代理服务器。在服务器终端设置相关环境变量：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7890</span><br></pre></td></tr></table></figure></div>

<p>由于配置代理是为了docker服务，所以还要编辑Docker系统的服务配置文件：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">mkdir</span> -p /etc/systemd/system/docker.service.d</span><br><span class="line"><span class="built_in">sudo</span> vim /etc/systemd/system/docker.service.d/proxy.conf</span><br></pre></td></tr></table></figure></div>

<p>在proxy.conf中配置以下内容：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">&quot;HTTP_PROXY=http://127.0.0.1:7890&quot;</span></span><br><span class="line">Environment=<span class="string">&quot;HTTPS_PROXY=http://127.0.0.1:7890&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>最后重启docker：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl daemon-reload</span><br><span class="line"><span class="built_in">sudo</span> systemctl restart docker</span><br></pre></td></tr></table></figure></div>

<p>不出意外应该成功了，可以尝试拉取一些之前拉取失败的镜像来验证一下</p>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>阿里云ECS</tag>
      </tags>
  </entry>
  <entry>
    <title>玩Maven后有但不多的一些心得</title>
    <url>/2024/11/16/Maven/</url>
    <content><![CDATA[<ul>
<li>Maven远程仓库默认为Maven Central Repository，可以在<code>/conf/settings.xml</code>中配置远程镜像仓库为国内阿里云镜像仓库，提高Jar包下载的速度和稳定性。配置settings.xml完成后保存退出。</li>
</ul>
<div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="code"><pre><span class="line"># 在settings.xml 160行处添加镜像源，需要保证镜像源标签在mirror的第一个</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://maven.aliyun.com/repository/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/16/67377c04abc81.png" alt="image-20241116005159364">像这样，镜像源得在前，否则会报错</p>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>“Maven”</tag>
        <tag>”阿里云ECS“</tag>
      </tags>
  </entry>
</search>
