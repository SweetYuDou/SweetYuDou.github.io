<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>RE-NET</title>
    <url>/2024/11/01/RE-Net/</url>
    <content><![CDATA[<h3 id="《Recurrent-Event-Network-Autoregressive-Structure-Inference-over-Temporal-Knowledge-Graphs》论文解读"><a href="#《Recurrent-Event-Network-Autoregressive-Structure-Inference-over-Temporal-Knowledge-Graphs》论文解读" class="headerlink" title="《Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs》论文解读"></a>《Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs》论文解读</h3><h4 id="RGCN"><a href="#RGCN" class="headerlink" title="RGCN"></a>RGCN</h4><p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238bf47b.png" alt="RGCN_formula"></p>
<p>W<sub>r</sub><sup>(l)</sup>是某一个特定的关系r（如弹劾，主动建交等）在第l层GCN中的权重矩阵，每一个不同的关系r对应其对应的权重矩阵，比如r<sub>1</sub>对应W<sub>1</sub>这样</p>
<p>h<sub>o</sub><sup>(l)</sup>是某一个特定的客体（object）在第l层GCN中的隐藏表示&#x2F;特征向量（hidden state&#x2F;embedding），也就是当前s实体所指向的那个o客体，<em><strong>h<sub>s</sub><sup>(l)</sup>同理</strong></em></p>
<p>W<sub>o</sub><sup>(l)</sup>是自环权重矩阵，意思就是它会与节点s自身的特征一直相乘然后更新到下一层再继续相乘</p>
<p>N<sub>t</sub><sup>(s,r)</sup>表示在时间戳t（timestamp）这个时间下的节点s通过关系r的邻居节点集合（Neighborhood），<em><strong>N<sub>t</sub><sup>(s)</sup>同理</strong></em></p>
<p>c<sub>s</sub>是归一化常数，体现在公式里就是：num(o) * num(r)，就是当前t的实体s对应的客体o的个数乘上当前t这个s对应的关系r的个数，包含了多少个邻居的信息就平均掉。<strong>简单来说，就是当前t的实体s的所有邻居个数</strong></p>
<p>RGCN聚合器的作用在于：聚合节点s的邻居节点的信息。结合公式解读一下就是：对于节点s，RGCN在更新节点特征同时考虑<strong>节点自身的特征</strong><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238ab7d7.png" alt="self_loop">和<strong>其邻居节点的特征</strong><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238b540f.png" alt="self_rel">，以及它们之间的关系类型（指不同关系对应不同权重矩阵)。</p>
<h4 id="RGCN模块代码如下："><a href="#RGCN模块代码如下：" class="headerlink" title="RGCN模块代码如下："></a>RGCN模块代码如下：</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RGCNLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_feat, out_feat, bias=<span class="literal">None</span>, activation=<span class="literal">None</span>, self_loop=<span class="literal">False</span>, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RGCNLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.bias = bias</span><br><span class="line">        <span class="variable language_">self</span>.activation = activation</span><br><span class="line">        <span class="variable language_">self</span>.self_loop = self_loop</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            <span class="variable language_">self</span>.bias = nn.Parameter(torch.Tensor(out_feat))</span><br><span class="line">            nn.init.xavier_uniform_(<span class="variable language_">self</span>.bias,  gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">            <span class="comment"># xavier_uniform_：初始化，用于保证输入输出的方差相同。可以避免随着层数的传递，输入过大而梯度消失或输入过小而失去非线性</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 权重矩阵</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">            <span class="comment"># 回环权重矩阵</span></span><br><span class="line">            <span class="comment"># input_feature,output_feature</span></span><br><span class="line">            <span class="variable language_">self</span>.loop_weight = nn.Parameter(torch.Tensor(in_feat, out_feat))</span><br><span class="line">            nn.init.xavier_uniform_(<span class="variable language_">self</span>.loop_weight, gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dropout:</span><br><span class="line">            <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dropout = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            g是graph，.ndata[&#x27;h&#x27;]是PyTorch DGL图（Graph）对象的一个节点数据属性，它表示图中的节点特征向量。</span></span><br><span class="line"><span class="string">            在 GNNs 中，节点特征向量通常由两部分组成：一部分是静态的特征，如节点类别、属性等；</span></span><br><span class="line"><span class="string">            另一部分是动态的特征，如与节点相邻的节点数量、节点之间的边权重等。</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            loop_message = torch.mm(g.ndata[<span class="string">&#x27;h&#x27;</span>], <span class="variable language_">self</span>.loop_weight)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                loop_message = <span class="variable language_">self</span>.dropout(loop_message)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.propagate(g, reverse)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用bias偏移和激活函数</span></span><br><span class="line">        node_repr = g.ndata[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            node_repr = node_repr + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">            node_repr = node_repr + loop_message</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.activation:</span><br><span class="line">            node_repr = <span class="variable language_">self</span>.activation(node_repr)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新节点特征向量</span></span><br><span class="line">        g.ndata[<span class="string">&#x27;h&#x27;</span>] = node_repr</span><br><span class="line">        <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RGCNBlockLayer</span>(<span class="title class_ inherited__">RGCNLayer</span>):  <span class="comment"># RGCN块层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_feat, out_feat, num_rels, num_bases, bias=<span class="literal">None</span>, activation=<span class="literal">None</span>, self_loop=<span class="literal">False</span>, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RGCNBlockLayer, <span class="variable language_">self</span>).__init__(in_feat, </span><br><span class="line">                                             out_feat, </span><br><span class="line">                                             bias,</span><br><span class="line">                                             activation, </span><br><span class="line">                                             self_loop=self_loop,</span><br><span class="line">                                             dropout=dropout)</span><br><span class="line">        <span class="variable language_">self</span>.num_rels = num_rels</span><br><span class="line">        <span class="variable language_">self</span>.num_bases = num_bases</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.num_bases &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.out_feat = out_feat</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.submat_in = in_feat // <span class="variable language_">self</span>.num_bases</span><br><span class="line">        <span class="variable language_">self</span>.submat_out = out_feat // <span class="variable language_">self</span>.num_bases</span><br><span class="line"></span><br><span class="line">        <span class="comment"># assuming in_feat and out_feat are both divisible by num_bases</span></span><br><span class="line">        <span class="comment"># if self.num_rels == 2:</span></span><br><span class="line">        <span class="comment">#     self.in_feat = in_feat</span></span><br><span class="line">        <span class="comment">#     self.weight = nn.Parameter(torch.Tensor(</span></span><br><span class="line">        <span class="comment">#         self.num_rels, in_feat, out_feat))</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.Tensor(<span class="variable language_">self</span>.num_rels, <span class="variable language_">self</span>.num_bases * <span class="variable language_">self</span>.submat_in * <span class="variable language_">self</span>.submat_out))</span><br><span class="line">        nn.init.xavier_uniform_(<span class="variable language_">self</span>.weight, gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">msg_func</span>(<span class="params">self, edges, reverse</span>):  <span class="comment"># 节点和权重相乘，再转化为输出格式</span></span><br><span class="line">        <span class="keyword">if</span> reverse:</span><br><span class="line">            <span class="comment"># edges.data[&#x27;type_o&#x27;]：表示目标节点所属的关系的类型。具体哪个关系就对应它的权重矩阵</span></span><br><span class="line">            weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_o&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_s&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        edges.src[&#x27;h&#x27;]：表示源节点的特征向量。</span></span><br><span class="line"><span class="string">        它是一个大小为 (num_nodes, submat_in) 的tensor，其中 num_nodes 表示图中的节点数。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        node = edges.src[<span class="string">&#x27;h&#x27;</span>].view(-<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.submat_in)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        msg：表示消息传递的结果。</span></span><br><span class="line"><span class="string">        它是一个大小为 (num_edges, out_feat) 的tensor，其中out_feat 表示输出特征向量的维度。</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        msg = torch.bmm(node, weight).view(-<span class="number">1</span>, <span class="variable language_">self</span>.out_feat)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;msg&#x27;</span>: msg&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">        g.update_all(<span class="keyword">lambda</span> x: <span class="variable language_">self</span>.msg_func(x, reverse), fn.<span class="built_in">sum</span>(msg=<span class="string">&#x27;msg&#x27;</span>, out=<span class="string">&#x27;h&#x27;</span>), <span class="variable language_">self</span>.apply_func)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply_func</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;h&#x27;</span>: nodes.data[<span class="string">&#x27;h&#x27;</span>] * nodes.data[<span class="string">&#x27;norm&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure></div>

<p>结合代码来看公式，是<img lazyload src="/images/loading.svg" data-src="/RE-Net/67376238ab7d7.png" alt="self_loop"></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">   		<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         先让节点特征向量与自环权重矩阵相乘+dropout</span></span><br><span class="line"><span class="string">         &#x27;&#x27;&#x27;</span></span><br><span class="line">loop_message = torch.mm(g.ndata[<span class="string">&#x27;h&#x27;</span>], <span class="variable language_">self</span>.loop_weight)</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">             loop_message = <span class="variable language_">self</span>.dropout(loop_message)</span><br><span class="line">         <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         再把偏移、乘积、激活函数加在原有的节点特征向量上</span></span><br><span class="line"><span class="string">         允许每一层都保留前一层的信息，有点像ResNet的思想</span></span><br><span class="line"><span class="string">         &#x27;&#x27;&#x27;</span></span><br><span class="line">	node_repr = g.ndata[<span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">             node_repr = node_repr + <span class="variable language_">self</span>.bias</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.self_loop:</span><br><span class="line">             node_repr = node_repr + loop_message</span><br><span class="line">         <span class="keyword">if</span> <span class="variable language_">self</span>.activation:</span><br><span class="line">             node_repr = <span class="variable language_">self</span>.activation(node_repr)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把最后的节点表示（node_repr）作为该层节点特征向量的输出</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">         g.ndata[<span class="string">&#x27;h&#x27;</span>] = node_repr</span><br><span class="line">         <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure></div>

<p><img lazyload src="/images/loading.svg" data-src="/RE-Net/67376238b540f.png" alt="self_rel">是</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">def</span> <span class="title function_">msg_func</span>(<span class="params">self, edges, reverse</span>): </span><br><span class="line">      <span class="keyword">if</span> reverse:</span><br><span class="line">          weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_o&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          weight = <span class="variable language_">self</span>.weight.index_select(<span class="number">0</span>, edges.data[<span class="string">&#x27;type_s&#x27;</span>]).view(-<span class="number">1</span>, <span class="variable language_">self</span>.submat_in, <span class="variable language_">self</span>.submat_out)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">先找到目标节点所属的关系的类型（比如：弹劾、被弹劾；选举、被选举）</span></span><br><span class="line"><span class="string">拿到该关系所对应的权重矩阵</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">      node = edges.src[<span class="string">&#x27;h&#x27;</span>].view(-<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>.submat_in)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将源节点的特征向量与权重相乘</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">      msg = torch.bmm(node, weight).view(-<span class="number">1</span>, <span class="variable language_">self</span>.out_feat)</span><br><span class="line">      <span class="keyword">return</span> &#123;<span class="string">&#x27;msg&#x27;</span>: msg&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">self, g, reverse</span>):</span><br><span class="line">      g.update_all(<span class="keyword">lambda</span> x: <span class="variable language_">self</span>.msg_func(x, reverse), fn.<span class="built_in">sum</span>(msg=<span class="string">&#x27;msg&#x27;</span>, out=<span class="string">&#x27;h&#x27;</span>), <span class="variable language_">self</span>.apply_func)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把该源节点的所有邻居节点的msg结果相加聚合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">apply_func</span>(<span class="params">self, nodes</span>):</span><br><span class="line">      <span class="keyword">return</span> &#123;<span class="string">&#x27;h&#x27;</span>: nodes.data[<span class="string">&#x27;h&#x27;</span>] * nodes.data[<span class="string">&#x27;norm&#x27;</span>]&#125;</span><br><span class="line">  	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  	最后乘上归一化常数nodes.data[&#x27;norm&#x27;]</span></span><br><span class="line"><span class="string">  	&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div>

<p>RGCN捕捉多跳邻居的信息：通过多次应用图卷积操作实现</p>
<ul>
<li>在第一次迭代中，每个节点聚合来自其直接邻居的信息</li>
<li>在第二次迭代中，每个节点合来自其邻居的邻居（即第二跳邻居）的信息</li>
</ul>
<h4 id="RE-Net架构图解读"><a href="#RE-Net架构图解读" class="headerlink" title="RE-Net架构图解读"></a>RE-Net架构图解读</h4><p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/67376238ebfd3.png" alt="RE-Net_architecture"></p>
<p>任务：预测时间戳t中的三元组</p>
<p>已知：过去几个时间步的图（架构图举例为3个时间步）</p>
<p>RE-Net_global对整个知识图谱（能观察到的图，架构图中整个图谱为3个时间步的图）进行编码，给出全局嵌入表示（global_emb）。在预测时给出预测时间戳t的主体、客体分布以及更新后的全局嵌入表示</p>
<p>RE-Net(train)：将可观察到的局部图中的s,r和global_emb等放入Aggregator聚合器（RGCN）当中，聚合器的输出结果为两个序列s_packed_input,s_packed_input_r分别放入encoder,encoder_r中，编码实体表示和关系表示，RE-Net类的self.encoder被定义为nn.GRU，最后分别送入线性层self.linear,self.linear_r(nn.linear)和dropout层预测客体和关系并分别计算损失</p>
<p>RE-Net(valid,test)：RE-Net_global预测时间戳t的主体，客体分布后采样前self.num_k个，更新（主体和客体的列表、索引、历史交互缓存），构建新的时间点的图，更新图字典（graph_dict）和全局嵌入表示（global_emb）。再重复以上操作↑</p>
]]></content>
      <categories>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker学习笔记</title>
    <url>/2024/11/14/Docker/</url>
    <content><![CDATA[<ul>
<li>docker镜像源（加速地址）是国内的源来拉镜像，可能有一些镜像没有，代理就是从Docker hub拉。<strong>你拉的镜像如果是<em>Dockerhub</em>的，并且国内镜像源没有，那镜像源也不好使，只能老老实实配合代理</strong>。阿里云ECS可以使用阿里云镜像加速器，<code>/etc/docker/daemon.json</code>如下配置即可，不要加其他源：</li>
</ul>
<div class="highlight-container" data-rel="Json"><figure class="iseeu highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;registry-mirrors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;https://***.mirror.aliyuncs.com&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>本地访问阿里云ECS的网页时，若出现无法访问的问题，可以检查该ECS配置的安全组是否开放HTTP（80）、HTTPS（443）端口。</li>
<li>用clash来给服务器<strong>配置代理</strong>，可以参考<a class="link" href="https://github.com/nelvko/clash-for-linux-install?tab=readme-ov-file">nelvko&#x2F;clash-for-linux-install: 优雅地部署基于 Clash 的代理环境 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>，其中订阅就是一个链接，可以从机场网站上找，类似这样的：</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/15/673762390d8ea.png" alt="subscription"></p>
<p>我这里用的是clash，就复制clash订阅链接就好了。部署完clash后，服务器开放端口7890作为代理接口，来访问代理服务器。在服务器终端设置相关环境变量：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 </span><br></pre></td></tr></table></figure></div>

<p>由于配置代理是为了docker服务，所以还要编辑Docker系统的服务配置文件：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">mkdir</span> -p /etc/systemd/system/docker.service.d</span><br><span class="line"><span class="built_in">sudo</span> vim /etc/systemd/system/docker.service.d/proxy.conf</span><br></pre></td></tr></table></figure></div>

<p>在proxy.conf中配置以下内容：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">&quot;HTTP_PROXY=http://127.0.0.1:7890&quot;</span></span><br><span class="line">Environment=<span class="string">&quot;HTTPS_PROXY=http://127.0.0.1:7890&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>最后重启docker：</p>
<div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl daemon-reload</span><br><span class="line"><span class="built_in">sudo</span> systemctl restart docker</span><br></pre></td></tr></table></figure></div>

<p>不出意外应该成功了，可以尝试拉取一些之前拉取失败的镜像来验证一下</p>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/20/673d70d56e2f1.png" alt="traffic"></p>
<ul>
<li>阿里云云数据传输的流量<strong>按量计算，两天就花了151.05CNY…TuT</strong>，目录价因子都是流出地域，我用Xftp7把ECS上的文件传到本地也用不到100多个G吧？暂时还没想到是为什么，只好先暂停服务了</li>
</ul>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>阿里云ECS</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Boot学习笔记</title>
    <url>/2024/11/16/Maven/</url>
    <content><![CDATA[<ul>
<li>Maven远程仓库默认为Maven Central Repository，可以在<code>/conf/settings.xml</code>中配置远程镜像仓库为国内阿里云镜像仓库，提高Jar包下载的速度和稳定性。配置settings.xml完成后保存退出。</li>
</ul>
<div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="code"><pre><span class="line"># 在settings.xml 160行处添加镜像源，需要保证镜像源标签在mirror的第一个</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://maven.aliyun.com/repository/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/16/67377c04abc81.png" alt="image-20241116005159364">像这样，镜像源得在前，否则会报错</p>
<ul>
<li>IntelliJ IDEA中项目代码若出现无法解析符号“xxx”报错且鼠标指针移到报错的符号（被高亮为红色）上没有提示”导入类’xxx‘时“，可以尝试点击右侧”更多操作“</li>
</ul>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/25/6743503f889cd.png" alt="更多操作"></p>
<p>点击“添加Maven依赖项”</p>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/25/6743503f852b2.png" alt="添加Maven依赖项"></p>
<p>再选择合适的版本</p>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/25/6743503f5ecf4.png" alt="选择合适版本"></p>
<p>最后再导入类，即可解决报错</p>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>阿里云ECS</tag>
        <tag>Maven</tag>
        <tag>学习笔记</tag>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title>对FPS游戏作弊工具的一些理解</title>
    <url>/2024/11/25/DMA/</url>
    <content><![CDATA[<p>这两天看到了个几张图片，据说是超越了DMA的作弊工具，看完后心血来潮想研究一下现在的一些游戏外挂的原理。</p>
<ul>
<li>首先是普通外挂软件。游戏数据大部分都存在本地内存中，比如敌人位置、血量、技能等。正常情况下你肯定看不到，但外挂软件直接访问本地内存，并将这些数据可视化。比如一些透视挂，就可以通过从本地内存中读取的数据来显示敌人所在位置等信息。游戏通常会拒绝这些外挂软件访问游戏数据，稍微好一点的外挂软件就会伪装成各种正规的软件来访问，但这种普通的内存挂基本都会封，只是时间问题而已</li>
<li>而DMA可以优化这一点。DMA全称为Direct Memory Access，即直接存储器访问。DMA能够做到<strong>在数据传输期间</strong>越过CPU来访问内存，那么就可以通过DMA把本地电脑的内存复制一份到另一台电脑上，然后在另一台电脑上运行外挂软件来实现外挂功能（如透视），而外挂软件完全不访问运行在本地的游戏数据，所以很难封禁。但是由于现代操作系统（如 Windows、Linux 等）都有复杂的内存管理机制。操作系统会对内存进行虚拟化，将物理内存映射到不同的虚拟地址空间。对于 DMA 访问内存，<strong>DMA需要CPU去解析内存再传输给DMA</strong>，它不能直接使用物理内存地址，因为操作系统可能已经对这些物理地址进行了重新映射。如果直接使用物理地址访问内存，可能会访问到错误的区域，因为它不知道操作系统的内存映射规则。因此DMA虽然可以避开CPU，但绕不过操作系统，存在被操作系统检测到的可能</li>
<li>于是现在又有了HMTT来进一步优化。HMTT全称为Hybrid Memory Trace Tool，即混合内存跟踪工具。HMTT通过插在DDR4接口的内存上，使用DIMM-snooping机制监听内存总线，绕开了CPU。HMTT在读取内存时<strong>系统内部没有任何痕迹</strong>，可以理解为内存中间人，可以在数据从一个地方传输到另一个地方的过程中介入并获取数据。因此HMTT也可以在将内存数据发送给主板前复制一份给另一台电脑，且无法被操作系统检测到。HMTT是纯物理层面的解决方案，不经过操作系统，因此不存在操作系统层面的检测可能╮(╯▽╰)╭</li>
</ul>
<p>将来问世的各种硬件各种技术难免会被用在研发外挂上，这是不可避免的。在我看来，其实DMA就已经很难查封了，没必要因为新的外挂技术而沮丧，反正反作弊都查不出来🤣🤣🤣</p>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/26/6744b13a9bc43.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/26/6744b13a9ed3e.jpg"></p>
<p><img lazyload src="/images/loading.svg" data-src="https://www.helloimg.com/i/2024/11/26/6744b13aa23bb.jpg"></p>
<p>这三张图是我在冲浪时看到的，值得一提的是：第一张图提到的DMA和HMTT的区别是有误的，DMA不需要通过CPU来访问内存，第二张图“DMA获取内存的方式”也是错的，不知道是不是为了销售故意这样做的😏</p>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>DMA</tag>
        <tag>HMTT</tag>
      </tags>
  </entry>
  <entry>
    <title>interview</title>
    <url>/2024/12/17/interview/</url>
    <content><![CDATA[<h4 id="12-17-UsingAI大模型算法实习生一面"><a href="#12-17-UsingAI大模型算法实习生一面" class="headerlink" title="12.17 UsingAI大模型算法实习生一面"></a>12.17 UsingAI大模型算法实习生一面</h4><ol>
<li>介绍项目</li>
<li>继承和封装是什么？</li>
<li>多线程是什么？</li>
<li>learning_rate过大会出现什么情况？</li>
<li>怎么处理过拟合？</li>
<li>l1和l2正则化是什么？有什么区别？</li>
<li>CNN卷积核越大越好吗？</li>
<li>对于很大的数据集，怎么提高决策树的效率？</li>
<li>交叉验证怎么找到k值？</li>
<li>文本分类的模型了解哪些？</li>
</ol>
<ul>
<li><p>约3-4天后联系二面</p>
</li>
<li><p>提问：几轮面试？</p>
<blockquote>
<p>回答：两轮技术面一轮leader面</p>
</blockquote>
</li>
<li><p>BTW一面的小姐姐声音还挺甜的:)</p>
</li>
</ul>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title>vpn</title>
    <url>/2024/11/30/vpn/</url>
    <content><![CDATA[<p>代理、VPN和加速器的区别</p>
<p>什么是代理（Proxy）？<br>代理服务器是一个网络服务，它允许用户通过它来转发网络请求和响应。简单来说，当你使用代理服务器时，你的网络请求先发送到这个代理服务器，由它代表你访问目标网站，然后再将网站的数据转发回你的设备。代理的种类繁多，常见的有HTTP代理、HTTPS代理和SOCKS代理。</p>
<p>代理的主要功能是帮助用户突破地理或网络访问限制，提高匿名性。但它们通常不会加密数据，因此安全性不如VPN。</p>
<p>VPN（虚拟私人网络）<br>VPN，即虚拟私人网络，能在公共网络上建立一条加密的私人通道，让用户的数据在传输过程中保持隐私和安全。当使用VPN时，用户的所有网络流量都会通过这个加密通道传输，不仅仅是浏览网页，包括所有应用产生的流量，如邮件、游戏和文件传输等。</p>
<p>VPN不仅可以用来翻墙访问被屏蔽的网站，还能保护用户在公共Wi-Fi下的上网安全，防止数据被窃取。</p>
<p>翻墙技术<br>翻墙是一个非正式的术语，通常指的是绕过互联网封锁，访问在某地区或国家被限制或屏蔽的网络内容。翻墙不是一种技术，而是一个包含多种技术的过程。使用代理和VPN都可以实现翻墙的目的，除此之外，还有一些专门为翻墙设计的工具，如Tor网络。</p>
<p>它们之间的联系与区别<br>尽管代理、VPN和翻墙技术在某些方面有所重叠，但它们之间还是存在一些本质的区别。代理更适用于快速访问特定服务，而VPN提供了更全面、更安全的网络访问解决方案。翻墙则是一个广泛的概念，涵盖了使用代理、VPN等多种技术来绕过网络限制。</p>
<p>总的来说，用户在选择这些工具时应根据自己的需求和所在地区的网络环境来做决定。安全性和隐私保护常常是选择VPN而不是简单代理的主要原因，而寻求特定服务的快速访问，则可能倾向于使用代理服务器。</p>
]]></content>
      <categories>
        <category>chit-chat</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>vpn</tag>
      </tags>
  </entry>
</search>
